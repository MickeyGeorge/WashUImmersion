{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST Practice\n",
    "\n",
    "### Load in the data set\n",
    "In this practice, we will be using XGBoost with Python to build a supervised learning system that predicts the housing prices of Boston in the past. \n",
    "\n",
    "This is an infamous dataset that has been used in many machine learning papers. The original data is constructed by the paper \"Hedonic prices and the demand for clean air\", J. Environ. Economics & Management, vol.5, 81-102, 1978.\n",
    "\n",
    "We fisrt load the dataset from sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a pandas dataset with the price and all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple XGBoost with CV\n",
    "Load in XGBoost Packages and select out features (the first 13 columns) and labels (the last column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then define the cross validation parameters and get the cross valudation results using [xgb.cv()](https://xgboost.readthedocs.io/en/latest/python/python_api.html).\n",
    "\n",
    "The important parameters that you need to define as:\n",
    "1. **learning_rate**: step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "2. **max_depth**: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "3. **objective**: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\n",
    "4. **alpha**: L1 regularization on leaf weights. A large value leads to more regularization.\n",
    "5. **colsample_bytree**: percentage of features used per tree. High value can lead to overfitting.\n",
    "\n",
    "\n",
    "Let us first use \"reg:lienar\" as objective, colsample_bytree = 0.3, learning_rate = 0.1, max_depth=5 and alpha=10.\n",
    "\n",
    "In the cross validation, you need to specify:\n",
    "1. **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "2. **metrics**: tells the evaluation metrics to be watched during CV\n",
    "3. **as_pandas**: to return the results in a pandas DataFrame.\n",
    "4. **early_stopping_rounds**: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "5. **seed**: for reproducibility of results.\n",
    "\n",
    "We will use num_boost_rounds=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, sead=123, nfolds=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the cross validation results using the dataset cv_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us then start tuing the parameters of XGBoost. For more information on which parameters to tune, read the following documents: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "In this part, you need to create a grid of parameters you want to search, and use CV to fine tune the parameters. You will then read your cross-validation results. \n",
    "\n",
    "You can use a for-loop to tune the parameters. You can also use \"from sklearn.grid_search import GridSearchCV\" to tune your parameters. (For-loop is recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
